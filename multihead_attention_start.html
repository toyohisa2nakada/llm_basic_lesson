<html>
  <!--
neko_editorã«ã‚³ãƒ”ãƒ¼ã™ã‚‹ç”¨ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ 
å®Ÿéš›ã«ä½¿ç”¨ã™ã‚‹ã¨ãã¯ã€Google Formã«è²¼ã‚Šä»˜ã‘ã¦å­¦ç”Ÿã¯ãã“ã‹ã‚‰ã‚³ãƒ”ãƒ¼ã—ã¦neko_editorã«è²¼ã‚Šä»˜ã‘ã‚‹ã€‚
neko_editorä¸Šã§å‹•ä½œã•ã›ã‚‹ãŸã‚ã€ä»–ã®jsãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‘¼ã³å‡ºã™ã¨ãã¯github pagesã®URLã¨ã™ã‚‹ã€‚
-->

  <head>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.22.0/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-vis@1.5.1"></script>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://llm_basic_lesson.pecode.com/MultiHeadAttention.js"></script>
    <script src="https://llm_basic_lesson.pecode.com/SliceLayer.js"></script>
    <script src="https://llm_basic_lesson.pecode.com/multihead_attention_internal.js"></script>
  </head>

  <body>
    <script>
      function generateFavoriteDatasets() {
        // ğŸš¨1 å·¦ãŒå¥½ããªã‚‚ã®ã€å³ãŒå«Œã„ãªã‚‚ã®ã€è‡ªåˆ†ã®å—œå¥½ã«åˆã‚ã›ã¦å¤‰æ›´ã™ã‚‹
        const objects = [["ãƒã‚±ãƒ¢ãƒ³", "ã‚²ãƒ¼ãƒ ", "ã‚«ãƒ¬ãƒ¼"], ["å¤§å­¦"]];

        const subjects = [
          ["ç§ã¯", ["å¥½ãã§ã™", "å«Œã„ã§ã™"]],
          ["ä¿ºã¯", ["å¥½ãã ", "å«Œã„ã "]],
        ];

        const test_patterns = [];
        const correct_answers = [];

        // ğŸš¨3 ã“ã®ä¸‹ã«ã‚³ãƒ”ãƒ¼ã™ã‚‹
        // const choice = (arr) => arr[Math.floor(Math.random() * arr.length)];
        // objects.forEach((objs, objs_i) => {
        //   const obj = choice(objs);
        //   subjects.forEach((sub) => {
        //     // é †åˆ—
        //     test_patterns.push([sub[0], obj]);
        //     correct_answers.push(sub[1][objs_i]);

        //     // é€†é †
        //     test_patterns.push([obj, sub[0]]);
        //     correct_answers.push(sub[1][objs_i]);
        //   });
        // });

        const sentences = [];
        objects
          .map((o, i) => o.map((oi) => [oi, i]))
          .flat()
          .forEach((oi) => {
            subjects.forEach(([sub, verbs]) => {
              sentences.push(`${sub} ${oi[0]} ${verbs[oi[1]]}`);
            });
          });
        return generateDatasets({
          sentences,
          test_patterns,
          correct_answers,
          mode: "next",
        });
      }

      function createSimpleFNN({
        vocabSize,
        inputDim,
        keyDim,
        learningRate,
        type,
      }) {
        // ğŸš¨2 1è¡Œå…¥ã‚Œæ›¿ãˆã‚‹
        // return undefined;

        const input = tf.input({
          shape: [inputDim],
          dtype: "int32",
          name: "char_input",
        });
        const charEmbed = tf.layers
          .embedding({ inputDim: vocabSize, outputDim: keyDim, maskZero: true })
          .apply(input);
        const flat = tf.layers.flatten().apply(charEmbed);
        const logits = tf.layers.dense({ units: vocabSize }).apply(flat);
        const output = tf.layers
          .activation({ activation: "softmax" })
          .apply(logits);
        const model = tf.model({
          inputs: input,
          outputs: output,
          name: `fnn(${type ?? ""})`,
        });
        model.compile({
          optimizer: tf.train.adam(learningRate),
          loss: "sparseCategoricalCrossentropy",
          metrics: ["accuracy"],
        });
        return { model };
      }
      function createSimpleGAP({
        vocabSize,
        inputDim,
        keyDim,
        learningRate,
        type,
      }) {
        // ğŸš¨4 1è¡Œå…¥ã‚Œæ›¿ãˆã‚‹
        return undefined;

        const input = tf.input({
          shape: [inputDim],
          dtype: "int32",
          name: "char_input",
        });
        const embedding = tf.layers.embedding({
          inputDim: vocabSize,
          outputDim: keyDim,
          maskZero: true,
        });
        const charEmbed = embedding.apply(input);
        const pooled = tf.layers.globalAveragePooling1d().apply(charEmbed);
        const logits = tf.layers.dense({ units: vocabSize }).apply(pooled);
        const output = tf.layers
          .activation({ activation: "softmax" })
          .apply(logits);
        const model = tf.model({
          inputs: input,
          outputs: output,
          name: `gap(${type})`,
        });
        model.compile({
          optimizer: tf.train.adam(learningRate),
          loss: "sparseCategoricalCrossentropy",
          metrics: ["accuracy"],
        });
        return { model };
      }
      function createSimpleLLM({
        vocabSize,
        inputDim,
        numHeads,
        keyDim,
        learningRate,
      }) {
        // ğŸš¨5 1è¡Œå…¥ã‚Œæ›¿ãˆã‚‹
        return undefined;

        const input = tf.input({
          shape: [inputDim],
          dtype: "int32",
          name: "char_input",
        });
        const embedding = tf.layers.embedding({
          inputDim: vocabSize,
          outputDim: keyDim,
          maskZero: true,
        });
        const charEmbed = embedding.apply(input);

        const mha = new MultiHeadAttention({ keyDim, numHeads });
        const attn = mha.apply([charEmbed, charEmbed, charEmbed]);
        const lastAttn = new SliceLayer({
          startIndex: charEmbed.shape[1] - 1,
        }).apply(attn);

        const lastEmbed = new SliceLayer({
          startIndex: charEmbed.shape[1] - 1,
        }).apply(charEmbed);

        const added = tf.layers.add().apply([lastAttn, lastEmbed]);
        const pooled = tf.layers.flatten().apply(added);
        const logits = tf.layers.dense({ units: vocabSize }).apply(pooled);
        const output = tf.layers
          .activation({ activation: "softmax" })
          .apply(logits);
        const model = tf.model({ inputs: input, outputs: output, name: `llm` });

        model.compile({
          optimizer: tf.train.adam(learningRate),
          loss: "sparseCategoricalCrossentropy",
          metrics: ["accuracy"],
        });
        return { model, options: { mha } };
      }

      setupVisor({ tfvis, setDatasets, learn });
    </script>
  </body>
</html>
